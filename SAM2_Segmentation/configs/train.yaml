# ============================================================
# SAM 2 Fine-tuning Configuration
# ============================================================

# --- Data ---
data:
  processed_dir: "../processed"       # Relative to SAM2_Segmentation/
  unprocessed_dir: "../unprocessed"
  crop_size: 1024                     # SAM expects 1024x1024 input
  original_size: [240, 320]           # Original image dimensions H, W
  num_workers: 4
  train_split: 0.85

# --- SAM 2 Model ---
model:
  model_cfg: "sam2.1/sam2.1_hiera_s.yaml"     # SAM 2.1 Hiera Small
  checkpoint: "checkpoints/sam2.1_hiera_small.pt"
  # Which components to fine-tune
  freeze_image_encoder: true          # Keep encoder frozen (saves memory)
  freeze_prompt_encoder: true         # Keep prompt encoder frozen
  train_mask_decoder: true            # Fine-tune mask decoder

# --- Training ---
training:
  epochs: 150
  batch_size: 4                       # SAM uses 1024x1024 images, needs more VRAM
  learning_rate: 1.0e-4               # LR for mask decoder
  encoder_lr: 1.0e-5                  # LR for encoder (if unfrozen later)
  weight_decay: 0.01
  warmup_epochs: 5
  unfreeze_encoder_epoch: 50          # Unfreeze encoder after this epoch
  encoder_unfreeze_lr: 5.0e-6         # LR for encoder after unfreezing
  grad_clip: 1.0
  
  # Loss weights
  focal_weight: 20.0                  # Focal loss weight (SAM default)
  dice_weight: 1.0                    # Dice loss weight
  iou_weight: 1.0                     # IoU prediction loss weight
  
  # Prompt strategy during training
  num_points_per_mask: 3              # Random points sampled from GT mask
  use_box_prompt: true                # Also use bounding box prompts
  box_noise_pixels: 10                # Add noise to box prompts for robustness

# --- Checkpointing ---
checkpoint:
  save_dir: "checkpoints"
  save_every: 5
  log_dir: "logs/training"

# --- Early Stopping ---
early_stopping:
  patience: 30
  min_delta: 0.001

# --- Misc ---
seed: 42
device: "cuda"
